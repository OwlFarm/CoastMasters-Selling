# ðŸš€ Production Deployment Guide: Enhanced Migration/Scraper Workflow

## **Overview**
This guide covers the deployment of our production-ready migration/scraper workflow that can extract yacht data from various listing websites with advanced validation and confidence scoring.

## **âœ… Features Implemented**

### **1. Real Web Scraper Service**
- **Python-based scraper** with Selenium + BeautifulSoup
- **Multi-format support**: De Valk, YachtWorld, Boats.com, generic sites
- **Chrome automation** for JavaScript-heavy sites
- **Fallback mechanisms** for robust operation

### **2. Enhanced Field Mapping**
- **Intelligent data parsing** for different yacht listing formats
- **Multiple source variations** (brand/make, model/series, etc.)
- **Validation helpers** for data quality assurance
- **Comprehensive field coverage** (dimensions, engine, accommodation, etc.)

### **3. Image Migration Support**
- **Automatic image extraction** from listing pages
- **Hero image selection** for primary display
- **URL normalization** (relative to absolute)
- **Image count limits** for performance

### **4. Advanced Validation & Confidence Scoring**
- **Field-by-field validation** with specific rules
- **Confidence scoring** based on data quality
- **Validation issue logging** for debugging
- **Bonus scoring** for images and complete data

## **ðŸ”§ Technical Architecture**

### **Frontend (Next.js)**
```
src/
â”œâ”€â”€ components/sell-form.tsx          # Enhanced form with migration
â”œâ”€â”€ services/migration-service.ts     # TypeScript migration service
â””â”€â”€ app/api/migrate/route.ts         # Migration API endpoint
```

### **Backend (Python Flask)**
```
python-scraper/
â”œâ”€â”€ app.py                           # Main Flask application
â”œâ”€â”€ scraper_service.py               # Core scraping logic
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ Dockerfile                      # Container configuration
```

## **ðŸš€ Deployment Steps**

### **Step 1: Deploy Python Scraper Service**

```bash
# Build and deploy scraper service
cd python-scraper
docker build -t yacht-scraper .
docker run -d -p 5000:5000 --name yacht-scraper yacht-scraper
```

### **Step 2: Configure Environment Variables**

```bash
# Production environment variables
export SCRAPER_SERVICE_URL=http://your-production-server:5000
export SCRAPER_TIMEOUT=30000
export MAX_IMAGES_PER_LISTING=10
export ENABLE_IMAGE_MIGRATION=true
```

### **Step 3: Deploy Next.js Application**

```bash
# Build production version
npm run build

# Deploy to your hosting platform
# (Vercel, Netlify, AWS, etc.)
```

### **Step 4: Configure Reverse Proxy (Optional)**

```nginx
# Nginx configuration for scraper service
server {
    listen 80;
    server_name your-domain.com;
    
    location /api/migrate {
        proxy_pass http://localhost:9002;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location /scraper/ {
        proxy_pass http://localhost:5000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## **ðŸ“Š Monitoring & Maintenance**

### **Health Checks**
```bash
# Check scraper service health
curl http://localhost:5000/health

# Check migration API health
curl http://localhost:9002/api/migrate/health
```

### **Log Monitoring**
```bash
# Monitor scraper logs
docker logs -f yacht-scraper

# Monitor application logs
tail -f /var/log/nextjs/app.log
```

### **Performance Metrics**
- **Scraping success rate**: Target >95%
- **Average response time**: Target <5 seconds
- **Data completeness**: Target >80%
- **Image extraction rate**: Target >70%

## **ðŸ”’ Security Considerations**

### **Rate Limiting**
```python
# Add to Flask app
from flask_limiter import Limiter
limiter = Limiter(app, key_func=get_remote_address)

@app.route('/webhook/v2/extract', methods=['POST'])
@limiter.limit("10 per minute")
def extract():
    # ... existing code
```

### **User Agent Rotation**
```python
# Already implemented in scraper_service.py
self.ua = UserAgent()
self.session.headers.update({'User-Agent': self.ua.random})
```

### **Respect robots.txt**
```python
# Add robots.txt checking
import urllib.robotparser
rp = urllib.robotparser.RobotFileParser()
rp.set_url(f"{base_url}/robots.txt")
rp.read()
if not rp.can_fetch("*", url):
    return {"error": "robots.txt disallows scraping"}
```

## **ðŸ§ª Testing & Quality Assurance**

### **Automated Testing**
```bash
# Run scraper tests
cd python-scraper
python -m pytest tests/

# Run migration service tests
npm run test:migration
```

### **Test URLs for Different Formats**
- **De Valk**: `https://www.devalk.nl/en/yacht/*`
- **YachtWorld**: `https://www.yachtworld.com/yacht/*`
- **Boats.com**: `https://www.boats.com/boats-for-sale/*`
- **Generic**: Any yacht listing URL

### **Quality Metrics**
- **Data accuracy**: Compare scraped vs. manual entry
- **Field coverage**: Percentage of form fields populated
- **Image quality**: Resolution and relevance of extracted images
- **Processing speed**: Time from URL to populated form

## **ðŸ“ˆ Scaling Considerations**

### **Horizontal Scaling**
```yaml
# docker-compose.yml for multiple scraper instances
version: '3.8'
services:
  scraper-1:
    build: ./python-scraper
    ports: ["5001:5000"]
  scraper-2:
    build: ./python-scraper
    ports: ["5002:5000"]
  scraper-3:
    build: ./python-scraper
    ports: ["5003:5000"]
```

### **Load Balancing**
```nginx
# Nginx upstream configuration
upstream scraper_services {
    server localhost:5001;
    server localhost:5002;
    server localhost:5003;
}
```

### **Queue Management**
```python
# Add Redis queue for large-scale operations
import redis
from rq import Queue

redis_conn = redis.Redis()
queue = Queue(connection=redis_conn)

# Queue migration jobs
job = queue.enqueue(scrape_yacht_listing, url)
```

## **ðŸŽ¯ Success Metrics**

### **Business Metrics**
- **Migration success rate**: >90%
- **Time saved per listing**: >15 minutes
- **User adoption rate**: >80% of sellers use migration
- **Data quality improvement**: >25% better than manual entry

### **Technical Metrics**
- **API response time**: <3 seconds average
- **Scraper uptime**: >99.5%
- **Error rate**: <2%
- **Data completeness**: >85% average

## **ðŸš¨ Troubleshooting**

### **Common Issues**

1. **Chrome Driver Issues**
   ```bash
   # Rebuild container with updated Chrome
   docker-compose down
   docker-compose build --no-cache
   docker-compose up -d
   ```

2. **Memory Issues**
   ```bash
   # Monitor memory usage
   docker stats yacht-scraper
   
   # Restart if needed
   docker restart yacht-scraper
   ```

3. **Rate Limiting**
   ```bash
   # Check if site is blocking requests
   curl -A "Mozilla/5.0" https://example.com
   ```

### **Debug Mode**
```python
# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Add detailed error reporting
try:
    result = scraper_service.scrape_yacht_listing(url)
except Exception as e:
    logger.error(f"Detailed error: {e}", exc_info=True)
```

## **âœ¨ Conclusion**

The enhanced migration/scraper workflow is now **production-ready** with:
- âœ… Real web scraping capabilities
- âœ… Advanced data validation
- âœ… Image migration support
- âœ… Comprehensive error handling
- âœ… Docker deployment support
- âœ… Monitoring and scaling considerations

**Ready for production deployment!** ðŸš€
