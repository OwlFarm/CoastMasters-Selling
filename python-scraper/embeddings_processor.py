#!/usr/bin/env python3
"""
ğŸš€ EMBEDDINGS PROCESSOR FOR YACHT RAG SYSTEM
Generates OpenAI embeddings for PDF chunks and stores them in Pinecone
"""

import os
import json
from openai import OpenAI
from typing import List, Dict, Any, Optional
from datetime import datetime
from pinecone import Pinecone
import time

class YachtEmbeddingsProcessor:
    def __init__(self):
        # Initialize OpenAI
        openai_api_key = os.getenv('OPENAI_API_KEY')
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=openai_api_key)
        
        # Initialize Pinecone
        self.pinecone_client = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
        self.index_name = 'coast-masters-yacht-rag'
        self.index = self.pinecone_client.Index(self.index_name)
        
        print("âœ… Yacht Embeddings Processor initialized!")
        print(f"ğŸ¤– OpenAI: Ready for embeddings")
        print(f"ğŸ¯ Pinecone Index: {self.index_name}")
    
    def load_processed_chunks(self, filename: str = "processed_pdf_chunks_156.json") -> List[Dict[str, Any]]:
        """Load the processed PDF chunks from JSON"""
        try:
            with open(filename, 'r') as f:
                data = json.load(f)
            
            chunks = data.get('chunks', [])
            print(f"ğŸ“š Loaded {len(chunks)} processed chunks from {filename}")
            return chunks
            
        except FileNotFoundError:
            print(f"âŒ File not found: {filename}")
            print("ğŸ’¡ Run pdf_processor.py first to create chunks")
            return []
        except Exception as e:
            print(f"âŒ Error loading chunks: {e}")
            return []
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate OpenAI embedding for text"""
        try:
            response = self.client.embeddings.create(
                model="text-embedding-ada-002",
                input=text
            )
            
            embedding = response.data[0].embedding
            print(f"ğŸ¤– Generated embedding: {len(embedding)} dimensions")
            return embedding
            
        except Exception as e:
            print(f"âŒ Error generating embedding: {e}")
            return None
    
    def process_chunks_to_embeddings(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process all chunks and generate embeddings"""
        print(f"\nğŸš€ GENERATING EMBEDDINGS FOR {len(chunks)} CHUNKS")
        print("=" * 50)
        
        processed_chunks = []
        
        for i, chunk in enumerate(chunks):
            print(f"\nğŸ”„ Processing chunk {i+1}/{len(chunks)}: {chunk['id']}")
            
            # Generate embedding
            embedding = self.generate_embedding(chunk['text'])
            if embedding:
                # Prepare for Pinecone
                processed_chunk = {
                    'id': chunk['id'],
                    'values': embedding,
                    'metadata': {
                        **chunk['metadata'],
                        'embedding_generated_at': datetime.now().isoformat(),
                        'embedding_model': 'text-embedding-ada-002',
                        'embedding_dimensions': len(embedding)
                    }
                }
                
                processed_chunks.append(processed_chunk)
                print(f"âœ… Embedding generated and prepared for Pinecone")
                
                # Rate limiting - OpenAI has limits
                time.sleep(0.1)
            else:
                print(f"âŒ Failed to generate embedding for chunk {i+1}")
        
        print(f"\nğŸ‰ EMBEDDINGS GENERATION COMPLETE!")
        print(f"ğŸ“Š Successfully processed: {len(processed_chunks)}/{len(chunks)} chunks")
        
        return processed_chunks
    
    def store_in_pinecone(self, processed_chunks: List[Dict[str, Any]]) -> bool:
        """Store embeddings in Pinecone index"""
        if not processed_chunks:
            print("âŒ No processed chunks to store")
            return False
        
        try:
            print(f"\nğŸš€ STORING {len(processed_chunks)} EMBEDDINGS IN PINECONE")
            print("=" * 50)
            
            # Prepare data for Pinecone upsert
            vectors = []
            for chunk in processed_chunks:
                vectors.append({
                    'id': chunk['id'],
                    'values': chunk['values'],
                    'metadata': chunk['metadata']
                })
            
            # Upsert to Pinecone
            self.index.upsert(vectors=vectors)
            
            print(f"âœ… Successfully stored {len(vectors)} embeddings in Pinecone!")
            print(f"ğŸ¯ Index: {self.index_name}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error storing in Pinecone: {e}")
            return False
    
    def store_in_pinecone_batched(self, processed_chunks: List[Dict[str, Any]], batch_size: int = 100) -> bool:
        """Store embeddings in Pinecone index using batching to handle large datasets"""
        if not processed_chunks:
            print("âŒ No processed chunks to store")
            return False
        
        try:
            print(f"\nğŸš€ STORING {len(processed_chunks)} EMBEDDINGS IN PINECONE (BATCHED)")
            print("=" * 50)
            
            total_stored = 0
            total_batches = (len(processed_chunks) + batch_size - 1) // batch_size
            
            for i in range(0, len(processed_chunks), batch_size):
                batch_num = (i // batch_size) + 1
                batch_end = min(i + batch_size, len(processed_chunks))
                batch_chunks = processed_chunks[i:batch_end]
                
                print(f"\nğŸ”„ Processing batch {batch_num}/{total_batches} ({len(batch_chunks)} chunks)")
                
                # Prepare batch data for Pinecone upsert
                vectors = []
                for chunk in batch_chunks:
                    vectors.append({
                        'id': chunk['id'],
                        'values': chunk['values'],
                        'metadata': chunk['metadata']
                    })
                
                # Upsert batch to Pinecone
                self.index.upsert(vectors=vectors)
                
                total_stored += len(vectors)
                print(f"âœ… Batch {batch_num} stored: {len(vectors)} embeddings")
                print(f"ğŸ“Š Total stored so far: {total_stored}/{len(processed_chunks)}")
            
            print(f"\nğŸ‰ ALL BATCHES STORED SUCCESSFULLY!")
            print(f"ğŸ“Š Total embeddings stored: {total_stored}")
            print(f"ğŸ¯ Index: {self.index_name}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error storing in Pinecone: {e}")
            return False
    
    def test_search(self, query: str = "yacht specifications", top_k: int = 5) -> List[Dict[str, Any]]:
        """Test search functionality with a sample query"""
        try:
            print(f"\nğŸ” TESTING SEARCH: '{query}'")
            print("=" * 30)
            
            # Generate embedding for query
            query_embedding = self.generate_embedding(query)
            if not query_embedding:
                print("âŒ Failed to generate query embedding")
                return []
            
            # Search Pinecone
            search_results = self.index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True,
                namespace=""
            )
            
            print(f"âœ… Search completed! Found {len(search_results.matches)} results")
            
            # Display results
            for i, match in enumerate(search_results.matches):
                print(f"\nğŸ“Š Result {i+1}:")
                print(f"   ID: {match.id}")
                print(f"   Score: {match.score:.4f}")
                print(f"   PDF: {match.metadata.get('pdf_name', 'Unknown')}")
                print(f"   Chunk: {match.metadata.get('chunk_index', 'Unknown')}")
                
                # Show first 100 characters of text
                text_preview = match.metadata.get('text', '')[:100]
                print(f"   Text: {text_preview}...")
            
            return search_results.matches
            
        except Exception as e:
            print(f"âŒ Search test failed: {e}")
            return []
    
    def get_index_stats(self) -> Dict[str, Any]:
        """Get statistics about the Pinecone index"""
        try:
            stats = self.index.describe_index_stats()
            print(f"\nğŸ“Š PINECONE INDEX STATISTICS")
            print("=" * 30)
            print(f"ğŸ¯ Index: {self.index_name}")
            print(f"ğŸ“Š Total vectors: {stats.total_vector_count}")
            print(f"ğŸŒ Dimension: {stats.dimension}")
            print(f"ğŸ“ Namespaces: {list(stats.namespaces.keys()) if stats.namespaces else 'None'}")
            
            return stats
            
        except Exception as e:
            print(f"âŒ Error getting index stats: {e}")
            return {}

def main():
    """Main processing function"""
    print("ğŸš€ YACHT EMBEDDINGS PROCESSING PIPELINE")
    print("=" * 50)
    
    try:
        # Initialize processor
        processor = YachtEmbeddingsProcessor()
        
        # Load processed chunks
        chunks = processor.load_processed_chunks()
        if not chunks:
            print("âŒ No chunks to process. Run pdf_processor.py first.")
            return
        
        # Generate embeddings
        processed_chunks = processor.process_chunks_to_embeddings(chunks)
        if not processed_chunks:
            print("âŒ No embeddings generated")
            return
        
        # Store in Pinecone
        success = processor.store_in_pinecone(processed_chunks)
        if not success:
            print("âŒ Failed to store in Pinecone")
            return
        
        # Get index statistics
        processor.get_index_stats()
        
        # Test search functionality
        processor.test_search("yacht specifications length beam draft")
        
        print(f"\nğŸ‰ SUCCESS! Your yacht RAG system is now operational!")
        print("ğŸ”„ Ready to process more PDFs and scale up!")
        
    except Exception as e:
        print(f"âŒ Processing failed: {e}")

if __name__ == "__main__":
    main()
